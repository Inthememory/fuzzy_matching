{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\audec\\Documents\\fuzzy_matching\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from utils.load_data import *\n",
    "from utils.pairwise_similarity import pairwise_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "monoprix_gold = gold('../data/raw/', 'monoprix', config).collect()\n",
    "franprix_gold = gold('../data/raw/', 'franprix', config).collect()\n",
    "auchan_gold = gold('../data/raw/', 'auchan', config).collect()\n",
    "\n",
    "datasets = [monoprix_gold, franprix_gold, auchan_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_init = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            dataset.select(pl.col(\"brand_desc_slug\"))\n",
    "            for dataset in datasets\n",
    "        ], \n",
    "        how=\"vertical\"\n",
    "        )\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6516\n",
      "(6516, 384)\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# 'all-mpnet-base-v2', 'all-MiniLM-L6-v2'\n",
    "\n",
    "sentences = (\n",
    "    dataset_init\n",
    "    .get_columns()[0].to_list()\n",
    ")\n",
    "print(len(sentences))\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "\n",
    "# ## Print the embeddings\n",
    "# for sentence, embedding in zip(sentences, embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (42_458_256, 3)\n",
      "┌────────────────────────────────┬────────────────────────────────┬────────────┐\n",
      "│ left_side                      ┆ right_side                     ┆ similarity │\n",
      "│ ---                            ┆ ---                            ┆ ---        │\n",
      "│ str                            ┆ str                            ┆ f64        │\n",
      "╞════════════════════════════════╪════════════════════════════════╪════════════╡\n",
      "│ EUROSPEN                       ┆ EUROSPEN                       ┆ 1.0        │\n",
      "│ AQUA                           ┆ AQUA                           ┆ 1.0        │\n",
      "│ CANARD                         ┆ CANARD                         ┆ 1.0        │\n",
      "│ PLANTA FIN                     ┆ PLANTA FIN                     ┆ 1.0        │\n",
      "│ …                              ┆ …                              ┆ …          │\n",
      "│ MCFARLANE                      ┆ BLANC DES HAUTES PYRENEES      ┆ -0.211125  │\n",
      "│ BLANC DES HAUTES PYRENEES      ┆ MCFARLANE                      ┆ -0.211125  │\n",
      "│ AKASHI                         ┆ COURCEL LE COMPTOIR DES FLASKS ┆ -0.216756  │\n",
      "│ COURCEL LE COMPTOIR DES FLASKS ┆ AKASHI                         ┆ -0.216756  │\n",
      "└────────────────────────────────┴────────────────────────────────┴────────────┘\n",
      "(42458256, 3)\n"
     ]
    }
   ],
   "source": [
    "cossim = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    cossim[:,i] = util.cos_sim(embeddings[i], embeddings[:])\n",
    "    # cossim[i:,i] = util.cos_sim(embeddings[i], embeddings[i:])\n",
    "\n",
    "df_cossim = pairwise_similarity(csr_matrix(cossim), sentences)\n",
    "df_cossim = (\n",
    "    df_cossim\n",
    "    .sort(by=['similarity'], descending = True)\n",
    " )\n",
    "print(df_cossim)\n",
    "print(df_cossim.shape)\n",
    "df_cossim.select(pl.col('left_side'), pl.col('right_side'), pl.col('similarity').alias('similarity_st')).write_csv('../data/ST_cossim.csv', separator=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compute cosine similarity between all pairs\n",
    "# cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# #Add all pairs to a list with their cosine similarity score\n",
    "# all_sentence_combinations = []\n",
    "# for i in range(len(cos_sim)-1):\n",
    "#     for j in range(i+1, len(cos_sim)):\n",
    "#         if cos_sim[i][j] < 0.999999:\n",
    "#             all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "# #Sort list by the highest cosine similarity score\n",
    "# all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# print(\"Top-10 most similar pairs:\")\n",
    "# for score, i, j in all_sentence_combinations[0:10]:\n",
    "#     print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.pairwise_similarity import pairwise_similarity\n",
    "# from utils.group_similar_strings import group_similar_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Two parameters to tune:\n",
    "# #min_cluster_size: Only consider cluster that have at least n elements\n",
    "# #threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\n",
    "\n",
    "# #Encode all sentences\n",
    "# embeddings_torch = model.encode(sentences, convert_to_tensor=True)\n",
    "# clusters = util.community_detection(embeddings_torch, min_community_size=1, threshold=0.85)\n",
    "\n",
    "# #Print clusters elements\n",
    "# clusters_dict = {}\n",
    "# for i, cluster in enumerate(clusters):\n",
    "#     for sentence_id in cluster:\n",
    "#         clusters_dict[sentences[sentence_id]] = i\n",
    "\n",
    "# clusters_cossim = (\n",
    "#     pl.DataFrame(list(zip(clusters_dict.keys(), clusters_dict.values())),\n",
    "#                  schema=['name', 'group_name'])\n",
    "# )\n",
    "# print(clusters_cossim)\n",
    "\n",
    "# clusters_cossim.write_csv('../datasets/ST_group_strings.csv', separator=\";\")\n",
    "        \n",
    "# # #Print elements for all clusters\n",
    "# # for i, cluster in enumerate(clusters):\n",
    "# #     print(\"\\nCluster {}, #{} Elements \".format(i+1, len(cluster)))\n",
    "# #     for sentence_id in cluster:\n",
    "# #         print(\"\\t\", sentences[sentence_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_compare = (\n",
    "#     pl.scan_csv('../datasets/SG_cossim.csv', separator = ';').rename({\"similairity\": \"similairity_SG\"})\n",
    "#     .join(pl.scan_csv('../datasets/ST_cossim.csv', separator = ';').rename({\"similairity\": \"similairity_ST\"}), \n",
    "#           on=['left_side', 'right_side'], \n",
    "#           how=\"outer\")\n",
    "#     .join(pl.scan_csv('../datasets/Nomenclature_cossim.csv', separator = ';').rename({\"similairity\": \"similairity_Nomenclature\"}), \n",
    "#           on=['left_side', 'right_side'], \n",
    "#           how=\"outer\")\n",
    "#     .collect()\n",
    "# )\n",
    "# df_compare.write_csv('../datasets/df_compare.csv', separator=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
