{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\audec\\Documents\\fuzzy_matching\\venv\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import yaml\n",
    "\n",
    "from slugify import slugify\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from utils.load_data import *\n",
    "from utils.pairwise_similarity import pairwise_similarity\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "monoprix_gold = gold('../data/', 'monoprix', config).collect()\n",
    "franprix_gold = gold('../data/', 'franprix', config).collect()\n",
    "auchan_gold = gold('../data/', 'auchan', config).collect()\n",
    "\n",
    "datasets = [monoprix_gold, franprix_gold, auchan_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99359, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>product_id</th><th>level_0</th><th>level_1</th><th>level_2</th><th>brand_desc_slug</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;0000000001649&quot;</td><td>null</td><td>&quot;PATISSERIE ET …</td><td>null</td><td>&quot;LITTLE MOONS M…</td></tr><tr><td>&quot;0000000007198&quot;</td><td>null</td><td>&quot;MELON ET PASTE…</td><td>null</td><td>&quot;BIO ENSEMBLE&quot;</td></tr><tr><td>&quot;0000000007256&quot;</td><td>null</td><td>&quot;BIO FRUITS ET …</td><td>null</td><td>&quot;BIO ENSEMBLE&quot;</td></tr><tr><td>&quot;0000000007395&quot;</td><td>null</td><td>&quot;AIL OIGNONS HE…</td><td>null</td><td>&quot;BIO ENSEMBLE&quot;</td></tr><tr><td>&quot;0000000007396&quot;</td><td>null</td><td>&quot;AIL OIGNONS HE…</td><td>null</td><td>&quot;BIO ENSEMBLE&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌───────────────┬─────────┬───────────────────────────────────┬─────────┬────────────────────┐\n",
       "│ product_id    ┆ level_0 ┆ level_1                           ┆ level_2 ┆ brand_desc_slug    │\n",
       "│ ---           ┆ ---     ┆ ---                               ┆ ---     ┆ ---                │\n",
       "│ str           ┆ str     ┆ str                               ┆ str     ┆ str                │\n",
       "╞═══════════════╪═════════╪═══════════════════════════════════╪═════════╪════════════════════╡\n",
       "│ 0000000001649 ┆ null    ┆ PATISSERIE ET VIENNOISERIE GLACE… ┆ null    ┆ LITTLE MOONS MOCHI │\n",
       "│ 0000000007198 ┆ null    ┆ MELON ET PASTEQUE                 ┆ null    ┆ BIO ENSEMBLE       │\n",
       "│ 0000000007256 ┆ null    ┆ BIO FRUITS ET LEGUMES DE SAISON   ┆ null    ┆ BIO ENSEMBLE       │\n",
       "│ 0000000007395 ┆ null    ┆ AIL OIGNONS HERBES AROMATIQUES    ┆ null    ┆ BIO ENSEMBLE       │\n",
       "│ 0000000007396 ┆ null    ┆ AIL OIGNONS HERBES AROMATIQUES    ┆ null    ┆ BIO ENSEMBLE       │\n",
       "└───────────────┴─────────┴───────────────────────────────────┴─────────┴────────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_init = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            dataset\n",
    "            .select(pl.col(\"product_id\"), pl.col(\"brand_desc_slug\").alias(f'brand_desc_slug_{i}'),\n",
    "                    pl.col(f\"level{config['classification_most_relevant_level']}\").alias(f\"level_{i}\"))\n",
    "            for i, dataset in enumerate(datasets)\n",
    "        ], \n",
    "        how=\"align\"\n",
    "        )\n",
    "    .with_columns(pl.concat_list([f'brand_desc_slug_{i}' for i, _ in enumerate(datasets)]).alias('brand_desc_slug'))\n",
    "    .drop([f'brand_desc_slug_{i}' for i, _ in enumerate(datasets)])\n",
    "    .explode(\"brand_desc_slug\")\n",
    "    .filter(pl.col('brand_desc_slug').is_not_null())\n",
    ")\n",
    "\n",
    "print(dataset_init.shape)\n",
    "dataset_init.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_LIST = stopwords.words('english') + stopwords.words('french')\n",
    "\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "def convert_to_list_of_words(list_of_sentences):\n",
    "    list_=[]\n",
    "    for s in list_of_sentences:\n",
    "        list_ += [''.join(char.lower() for char in item\n",
    "                          if char not in string.punctuation and len(char)>0)\n",
    "                          for item in s.split()]\n",
    "    return list_\n",
    "\n",
    "def lemmatize_words (list_of_words):\n",
    "    words_w_stopwords = [i for i in list_of_words if i not in STOPWORDS_LIST]\n",
    "    return [lemmatizer.lemmatize(w) for w in words_w_stopwords]\n",
    "\n",
    "def remove_duplicates(l):\n",
    "    return (list(set(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>brand_desc_slug</th><th>level</th><th>level_lemmatize</th><th>level_slug</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;LE RUSTIQUE&quot;</td><td>&quot;fromage bries …</td><td>&quot;fromage brie b…</td><td>&quot;FROMAGE BRIE B…</td></tr><tr><td>&quot;KER DIOP&quot;</td><td>&quot;graine bio fru…</td><td>&quot;graine bio fru…</td><td>&quot;GRAINE BIO FRU…</td></tr><tr><td>&quot;CHIPSTER&quot;</td><td>&quot;chips biscuit …</td><td>&quot;chips biscuit …</td><td>&quot;CHIPS BISCUIT …</td></tr><tr><td>&quot;CHEMINETT&quot;</td><td>&quot;barbecue&quot;</td><td>&quot;barbecue&quot;</td><td>&quot;BARBECUE&quot;</td></tr><tr><td>&quot;FERMIER D AUVE…</td><td>&quot;volaille&quot;</td><td>&quot;volaille&quot;</td><td>&quot;VOLAILLE&quot;</td></tr><tr><td>&quot;MONOPRIX GOURM…</td><td>&quot;baguette&quot;</td><td>&quot;baguette&quot;</td><td>&quot;BAGUETTE&quot;</td></tr><tr><td>&quot;YOPLAIT PERLE …</td><td>&quot;aux fruits aro…</td><td>&quot;fruit aromatis…</td><td>&quot;FRUIT AROMATIS…</td></tr><tr><td>&quot;CANDEREL SUGAR…</td><td>&quot;sucre&quot;</td><td>&quot;sucre&quot;</td><td>&quot;SUCRE&quot;</td></tr><tr><td>&quot;LIV&quot;</td><td>&quot;classique plat…</td><td>&quot;classique plat…</td><td>&quot;CLASSIQUE PLAT…</td></tr><tr><td>&quot;GEFEN&quot;</td><td>&quot;casher&quot;</td><td>&quot;casher&quot;</td><td>&quot;CASHER&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌───────────────────────┬────────────────────────┬────────────────────────┬────────────────────────┐\n",
       "│ brand_desc_slug       ┆ level                  ┆ level_lemmatize        ┆ level_slug             │\n",
       "│ ---                   ┆ ---                    ┆ ---                    ┆ ---                    │\n",
       "│ str                   ┆ str                    ┆ str                    ┆ str                    │\n",
       "╞═══════════════════════╪════════════════════════╪════════════════════════╪════════════════════════╡\n",
       "│ LE RUSTIQUE           ┆ fromage bries brie     ┆ fromage brie brie      ┆ FROMAGE BRIE BRIE      │\n",
       "│                       ┆ regional coul…         ┆ regional coulo…        ┆ REGIONAL COULO…        │\n",
       "│ KER DIOP              ┆ graine bio fruit sec   ┆ graine bio fruit sec   ┆ GRAINE BIO FRUIT SEC   │\n",
       "│ CHIPSTER              ┆ chips biscuit aperitif ┆ chips biscuit aperitif ┆ CHIPS BISCUIT APERITIF │\n",
       "│ CHEMINETT             ┆ barbecue               ┆ barbecue               ┆ BARBECUE               │\n",
       "│ …                     ┆ …                      ┆ …                      ┆ …                      │\n",
       "│ YOPLAIT PERLE DE LAIT ┆ aux fruits aromatise   ┆ fruit aromatise yaourt ┆ FRUIT AROMATISE YAOURT │\n",
       "│                       ┆ yaourt                 ┆                        ┆                        │\n",
       "│ CANDEREL SUGARLY      ┆ sucre                  ┆ sucre                  ┆ SUCRE                  │\n",
       "│ LIV                   ┆ classique plat         ┆ classique plat         ┆ CLASSIQUE PLAT         │\n",
       "│ GEFEN                 ┆ casher                 ┆ casher                 ┆ CASHER                 │\n",
       "└───────────────────────┴────────────────────────┴────────────────────────┴────────────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_init_concat = (\n",
    "    dataset_init\n",
    "    .select([\"brand_desc_slug\"]\n",
    "           + [pl.col(c) for c in dataset_init.columns if c.startswith('level') and not c.endswith('2')] ## drop Franprix classification\n",
    "            )\n",
    "    .unique() \n",
    "    .with_columns(pl.concat_list([c for c in dataset_init.columns if c.startswith('level') and not c.endswith('2')]).alias(f\"level\"))\n",
    "    .select('brand_desc_slug', 'level')\n",
    "    .explode(f\"level\")\n",
    "    .filter(pl.col(f\"level\").is_not_null())\n",
    "    .unique()\n",
    "    .groupby('brand_desc_slug')\n",
    "    .agg(pl.col(f\"level\"))    \n",
    "    .with_columns(pl.col(f\"level\").apply(convert_to_list_of_words))\n",
    "    .with_columns(pl.col(f\"level\").apply(remove_duplicates))\n",
    "    .with_columns(pl.col(f\"level\").apply(lemmatize_words).alias(f\"level_lemmatize\"))\n",
    "    .with_columns(pl.col(f'level').cast(pl.List(pl.Utf8)).list.join(\" \"))  \n",
    "    .with_columns(pl.col(f'level_lemmatize').cast(pl.List(pl.Utf8)).list.join(\" \"))  \n",
    "    .with_columns(\n",
    "        pl.col(f'level_lemmatize')\n",
    "        .apply(\n",
    "            lambda x: slugify(x, separator=\" \")\n",
    "            .upper()\n",
    "            .strip()\n",
    "        )\n",
    "        .alias(f'level_slug')\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset_init_concat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3426, 833)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 833)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>abbaye</th><th>abricot</th><th>absorbeur</th><th>accessoire</th><th>adhesifs</th><th>ado</th><th>adoucissant</th><th>agrume</th><th>aide</th><th>ail</th><th>air</th><th>alcool</th><th>aliment</th><th>alimentaire</th><th>alimentation</th><th>allege</th><th>allegee</th><th>alleges</th><th>allume</th><th>allumette</th><th>alternatifs</th><th>aluminium</th><th>ambiante</th><th>ambree</th><th>ambrees</th><th>americains</th><th>ampoule</th><th>amsterdam</th><th>anchois</th><th>andouillette</th><th>anglaise</th><th>animal</th><th>anise</th><th>anti</th><th>antipasti</th><th>aperitif</th><th>aperitifs</th><th>&hellip;</th><th>ultra</th><th>usa</th><th>usage</th><th>ustensile</th><th>vaisselle</th><th>vanille</th><th>vaporisateur</th><th>veau</th><th>vegan</th><th>vegetal</th><th>vegetale</th><th>vegetales</th><th>vegetariens</th><th>vegetaux</th><th>velo</th><th>veloutes</th><th>vermicelle</th><th>verre</th><th>vert</th><th>viande</th><th>vichy</th><th>viennoiserie</th><th>viennoiseries</th><th>vin</th><th>vinaigre</th><th>vinaigrette</th><th>visage</th><th>vitalite</th><th>vitamine</th><th>vitre</th><th>vodka</th><th>volaille</th><th>wc</th><th>whisky</th><th>wrap</th><th>yaourt</th><th>yeux</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&hellip;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&hellip;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.591432</td><td>0.0</td><td>&hellip;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&hellip;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&hellip;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 833)\n",
       "┌────────┬─────────┬───────────┬────────────┬───┬────────┬──────┬────────┬──────┐\n",
       "│ abbaye ┆ abricot ┆ absorbeur ┆ accessoire ┆ … ┆ whisky ┆ wrap ┆ yaourt ┆ yeux │\n",
       "│ ---    ┆ ---     ┆ ---       ┆ ---        ┆   ┆ ---    ┆ ---  ┆ ---    ┆ ---  │\n",
       "│ f64    ┆ f64     ┆ f64       ┆ f64        ┆   ┆ f64    ┆ f64  ┆ f64    ┆ f64  │\n",
       "╞════════╪═════════╪═══════════╪════════════╪═══╪════════╪══════╪════════╪══════╡\n",
       "│ 0.0    ┆ 0.0     ┆ 0.0       ┆ 0.0        ┆ … ┆ 0.0    ┆ 0.0  ┆ 0.0    ┆ 0.0  │\n",
       "│ 0.0    ┆ 0.0     ┆ 0.0       ┆ 0.0        ┆ … ┆ 0.0    ┆ 0.0  ┆ 0.0    ┆ 0.0  │\n",
       "│ 0.0    ┆ 0.0     ┆ 0.0       ┆ 0.0        ┆ … ┆ 0.0    ┆ 0.0  ┆ 0.0    ┆ 0.0  │\n",
       "│ 0.0    ┆ 0.0     ┆ 0.0       ┆ 0.0        ┆ … ┆ 0.0    ┆ 0.0  ┆ 0.0    ┆ 0.0  │\n",
       "│ 0.0    ┆ 0.0     ┆ 0.0       ┆ 0.0        ┆ … ┆ 0.0    ┆ 0.0  ┆ 0.0    ┆ 0.0  │\n",
       "└────────┴─────────┴───────────┴────────────┴───┴────────┴──────┴────────┴──────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab the column to group (values must be Unicode) Unique ? \n",
    "dataset = dataset_init_concat[f'level_slug']\n",
    "\n",
    "## generate the matrix of TF-IDF values for each item - Ngram\n",
    "vectorizer_ngram = TfidfVectorizer(stop_words=STOPWORDS_LIST, analyzer='word', token_pattern=r'(?u)\\b[A-Za-z]{2,}\\b')\n",
    "tf_idf_matrix_ngram = vectorizer_ngram.fit_transform(dataset)\n",
    "\n",
    "tfidf_tokens_ngram = vectorizer_ngram.get_feature_names_out()\n",
    "\n",
    "df_tfidfvect_ngram = pl.DataFrame(data=tf_idf_matrix_ngram.toarray(), schema=tfidf_tokens_ngram.tolist())\n",
    "\n",
    "print(tf_idf_matrix_ngram.shape)\n",
    "\n",
    "df_tfidfvect_ngram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3426\n"
     ]
    }
   ],
   "source": [
    "## Create list of brands\n",
    "name_vector = (\n",
    "    dataset_init_concat\n",
    "    .get_columns()[0].to_list()\n",
    ")\n",
    "print(len(name_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nb components: 266'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=0.9)\n",
    "tf_idf_matrix_ngram_pca = pca.fit_transform(df_tfidfvect_ngram)\n",
    "\n",
    "(f\"Nb components: {pca.n_components_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911316 911316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00449974, -0.00345188, -0.00519737, ...,  0.0051167 ,\n",
       "        0.01125158, -0.0039131 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_matrix = csr_matrix(tf_idf_matrix_ngram_pca)\n",
    "\n",
    "print(len(concat_matrix.nonzero()[0]), len(concat_matrix.nonzero()[1]))\n",
    "concat_matrix.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   0,    0,    0, ..., 3425, 3425, 3425]), array([   0,    1,    2, ..., 3423, 3424, 3425]))\n",
      "11737476 11737476\n",
      "[ 1.         -0.02401624 -0.01837458 ... -0.02492766 -0.01320667\n",
      "  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity matrix\n",
    "cosine_similarity = cosine_similarity(concat_matrix)\n",
    "cosine_similarity_csr = csr_matrix(cosine_similarity) \n",
    "\n",
    "print(cosine_similarity_csr.nonzero())\n",
    "print(len(cosine_similarity_csr.nonzero()[0]), len(cosine_similarity_csr.nonzero()[1]))\n",
    "print(cosine_similarity_csr.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (11_737_476, 3)\n",
      "┌──────────────────────────────┬──────────────────────────────┬────────────┐\n",
      "│ left_side                    ┆ right_side                   ┆ similarity │\n",
      "│ ---                          ┆ ---                          ┆ ---        │\n",
      "│ str                          ┆ str                          ┆ f64        │\n",
      "╞══════════════════════════════╪══════════════════════════════╪════════════╡\n",
      "│ FRISK CLEAN BEATH            ┆ FRISK CLEAN BEATH            ┆ 1.0        │\n",
      "│ NESTLE                       ┆ NESTLE                       ┆ 1.0        │\n",
      "│ HARTLEY S                    ┆ HARTLEY S                    ┆ 1.0        │\n",
      "│ ROGE CAVAILLES PARAPHARMACIE ┆ ROGE CAVAILLES PARAPHARMACIE ┆ 1.0        │\n",
      "│ …                            ┆ …                            ┆ …          │\n",
      "│ MUSTELA PARAPHARMACIE        ┆ MONOPRIX                     ┆ -0.507178  │\n",
      "│ MONOPRIX                     ┆ MUSTELA PARAPHARMACIE        ┆ -0.507178  │\n",
      "│ MONOPRIX                     ┆ MUSTELA                      ┆ -0.507178  │\n",
      "│ MUSTELA                      ┆ MONOPRIX                     ┆ -0.507178  │\n",
      "└──────────────────────────────┴──────────────────────────────┴────────────┘\n",
      "(11737476, 3)\n"
     ]
    }
   ],
   "source": [
    "df_cossim = pairwise_similarity(cosine_similarity_csr, name_vector)\n",
    "df_cossim = (\n",
    "    df_cossim\n",
    "    .sort(by=['similarity'], descending = True)\n",
    " )\n",
    " \n",
    "print(df_cossim)\n",
    "print(df_cossim.shape)\n",
    "\n",
    "df_cossim.select(pl.col('left_side'), pl.col('right_side'), pl.col('similarity').alias('similarity_sg')).write_csv('../temp_folder/Nomenclature_words_cossim.csv', separator=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
