<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>main API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>main</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import polars as pl
import numpy as np
import sys
import yaml
import argparse
from loguru import logger
import pickle

from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer

from src import Dataset, DatasetsMerged, Similarity
from src import (
    create_input_for_prediction,
    launch_training,
    get_predictions,
    evaluate_model,
    group_similar_strings,
    add_master_brand,
)


# Initialise paths
DATA_RAW_PATH = &#34;data/raw/&#34;
DATA_PROCESSED_PATH = &#34;data/processed/&#34;
MODELS_PATH = &#34;models/&#34;
MODEL_NAME = &#34;xgb_model&#34;

# Initialise logs format
logger.remove(0)
logger.add(
    sys.stderr,
    format=&#34;{time} | {level} | {message} | {extra}&#34;,
)


if __name__ == &#34;__main__&#34;:
    # Add argparse for the command line:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        &#34;--datasets&#34;, type=str, required=True, nargs=&#34;+&#34;, help=&#34;Datasets listing.&#34;
    )
    parser.add_argument(
        &#34;--training&#34;,
        default=True,
        action=argparse.BooleanOptionalAction,
        help=&#34;Execute training.&#34;,
    )
    args = parser.parse_args()

    # Load the configuration file and set parameters
    logger.info(&#34;Loading YAML configuration file&#34;)
    with open(&#34;config.yml&#34;, &#34;r&#34;) as file:
        config = yaml.safe_load(file)

    unknown_brands = config[&#34;generic_words&#34;]
    generic_words = config[&#34;generic_words&#34;]
    list_stopwords = [word for word in stopwords.words(&#34;french&#34;) if len(word) &gt; 1]
    lemmatizer = FrenchLefffLemmatizer()
    sl_replacements = config[&#34;sl_replacements&#34;]

    ## 1. Preprocessing: calculate input datasets 
    # Create a list of clean datasets to proceed
    logger.info(f&#34;Create datasets : {args.datasets}&#34;)
    datasets = [
        Dataset(
            pl.read_parquet(f&#34;data/raw/{dataset}.parquet&#34;),
            dataset,
            nb_levels=config[&#34;retailer&#34;][dataset][&#34;nb_levels&#34;],
            replacements_brand=[[&#34;&amp;&#34;, &#34;et&#34;]],
        ).filter_dataset(unknown_brands=unknown_brands)
        for dataset in args.datasets
    ]
    datasets_merged = DatasetsMerged(datasets)

    # Merge datasets and convert classification variable into dummy variables.
    brand_classification_dummy = datasets_merged.get_brand_classification_dummy(
        levels=config[&#34;classification_levels&#34;]
    )
    print(f&#34;brand_classification_dummy : {brand_classification_dummy.shape}&#34;)

    # Merge datasets and clean a specified level column.
    brand_classification_words = datasets_merged.get_brand_classification_words(
        level=config[&#34;classification_most_relevant_level&#34;],
        lemmatizer=lemmatizer,
        list_stopwords=list_stopwords,
    )
    print(f&#34;brand_classification_words : {brand_classification_words.shape}&#34;)

    # Concat vertically brands from each dataset
    brands_updated = datasets_merged.extract_brands(
        lemmatizer=lemmatizer,
        list_stopwords=list_stopwords,
        generic_words=generic_words,
        replacements=sl_replacements,
    )
    print(f&#34;brands_updated : {brands_updated.shape}&#34;)

    # Create all pairs of brands with a cartesian product
    brands_cross_join = DatasetsMerged.cross_join(
        brands_updated, [&#34;brand_desc_slug&#34;, &#34;brand_desc_slug_updated&#34;]
    )
    print(f&#34;brands_cross_join : {brands_cross_join.shape}&#34;)

    # Pair brands with similar products
    brands_with_same_products_paired = datasets_merged.pair_brands_with_same_products()
    print(
        f&#34;brands_with_same_products_paired : {brands_with_same_products_paired.shape}&#34;
    )
    brands_with_same_products_paired.write_csv(
        f&#34;{DATA_PROCESSED_PATH}brands_with_same_products_paired.csv&#34;, separator=&#34;;&#34;
    )

    ## 2. Build similarity features
    logger.info(&#34;Build features&#34;)
    # Initialise an empty list to stores similarity datasets
    similarities_features = []

    logger.info(&#34;similarity_syntax_ngram&#34;)
    # Create Similarity object
    similarity_syntax_ngram = Similarity(
        brands_updated,
        name=&#34;syntax_ngram&#34;,
        label_col=&#34;brand_desc_slug&#34;,
        col=&#34;brand_desc_slug_updated_w_space&#34;,
        tfidf_required=True,
    )
    # Fix parameters
    similarity_syntax_ngram.analyzer = &#34;char&#34;
    similarity_syntax_ngram.ngram_range = (2, 3)
    # Compute cosin similarity
    similarity_syntax_ngram.cos_sim(min_similarity=0.2)
    similarities_features.append(similarity_syntax_ngram.pairwise_dataset)
    print(f&#34;sparsity : {similarity_syntax_ngram.sparsity()}&#34;)
    print(similarity_syntax_ngram.pairwise_dataset.shape)

    logger.info(&#34;similarity_classification&#34;)
    # Create Similarity object
    similarity_classification = Similarity(
        brand_classification_dummy, name=&#34;classification&#34;, label_col=&#34;brand_desc_slug&#34;
    )
    # Compute cosin similarity
    similarity_classification.cos_sim()
    similarities_features.append(similarity_classification.pairwise_dataset)
    print(f&#34;sparsity : {similarity_classification.sparsity()}&#34;)

    logger.info(&#34;similarity_classification_words&#34;)
    # Create Similarity object
    similarity_classification_words = Similarity(
        brand_classification_words,
        name=&#34;classification_words&#34;,
        label_col=&#34;brand_desc_slug&#34;,
        col=&#34;level_updated&#34;,
        tfidf_required=True,
    )
    # Fix parameters
    similarity_classification_words.token_pattern = r&#34;(?u)\b[A-Za-z]{2,}\b&#34;
    # Compute cosin similarity
    similarity_classification_words.cos_sim()
    similarities_features.append(similarity_classification_words.pairwise_dataset)
    print(f&#34;sparsity : {similarity_classification_words.sparsity()}&#34;)

    ## 3. Create input for prediction
    similarities_features.append(
        brands_cross_join.rename({&#34;brand_desc_slug_left&#34;: &#34;left_side&#34;}).rename(
            {&#34;brand_desc_slug_right&#34;: &#34;right_side&#34;}
        )
    )

    input_prediction_init = create_input_for_prediction(similarities_features)
    print(f&#34;input_prediction_init: {input_prediction_init.shape}&#34;)

    # Add distance_metrics
    logger.info(&#34;similarity_fuzzy&#34;)
    input_prediction_completed = Similarity.distance_metrics(
        input_prediction_init,
        col_left=&#34;brand_desc_slug_updated_left&#34;,
        col_right=&#34;brand_desc_slug_updated_right&#34;,
    )
    print(f&#34;input_prediction_completed: {input_prediction_completed.shape}&#34;)
    input_prediction_completed.write_csv(
        f&#34;{DATA_PROCESSED_PATH}input_prediction_completed.csv&#34;, separator=&#34;;&#34;
    )
    input_prediction_completed = input_prediction_completed.drop(
        &#34;brand_desc_slug_updated_left&#34;, &#34;brand_desc_slug_updated_right&#34;
    )

    ## 4. Fit the model (XGBoost Classifier) if training = True
    # Set variables
    indicators_var, label_var, target_var = (
        config[&#34;indicators_var&#34;],
        config[&#34;label_var&#34;],
        config[&#34;target_var&#34;],
    )

    if args.training:
        logger.info(&#34;Training of XGBoost Classifier&#34;)
        labeled_pairs = pl.read_csv(
            f&#34;{DATA_PROCESSED_PATH}training_dataset.csv&#34;, separator=&#34;;&#34;
        )

        # Make predictions and evaluate model
        xgb_model, test_predictions = launch_training(
            input_prediction_completed,
            labeled_pairs,
            indicators_var,
            label_var,
            target_var,
        )

        test_predictions.write_csv(
            f&#34;{DATA_PROCESSED_PATH}xgb_model_predictions_test.csv&#34;, separator=&#34;;&#34;
        )

        # Save model
        pickle.dump(xgb_model, open(f&#34;{MODELS_PATH}{MODEL_NAME}.pickle&#34;, &#34;wb&#34;))

    ## 5. Evaluate the model on a validation dataset
    val_dataset = input_prediction_completed.join(
        pl.read_csv(f&#34;{DATA_PROCESSED_PATH}validation_dataset.csv&#34;, separator=&#34;;&#34;),
        on=[&#34;left_side&#34;, &#34;right_side&#34;],
        how=&#34;inner&#34;,
    )
    val_predictions = get_predictions(xgb_model, val_dataset, config)

    val_predictions.write_csv(
        f&#34;{DATA_PROCESSED_PATH}xgb_model_predictions_val.csv&#34;, separator=&#34;;&#34;
    )

    log_loss_val, roc_auc_score_val, confusion_matrix_val = evaluate_model(
        xgb_model,
        val_dataset.select(indicators_var),
        val_dataset.select(target_var),
        &#34;val&#34;,
    )

    ## 6. Make predictions on the whole dataset
    logger.info(&#34;Predict&#34;)
    xgb_model = pickle.load(open(f&#34;{MODELS_PATH}{MODEL_NAME}.pickle&#34;, &#34;rb&#34;))

    predictions = get_predictions(xgb_model, input_prediction_completed, config)
    print(predictions.filter(pl.col(&#34;prediction&#34;) == 1).shape[0] / predictions.shape[0])
    predictions.write_csv(
        f&#34;{DATA_PROCESSED_PATH}xgb_model_predictions.csv&#34;, separator=&#34;;&#34;
    )

    ## 7. Create groups
    logger.info(&#34;Create groups&#34;)
    # Load predictions
    df_input_groups = (
        pl.concat(
            [
                predictions.select(
                    pl.col(&#34;left_side&#34;),
                    pl.col(&#34;right_side&#34;),
                    pl.col(&#34;proba_1&#34;).cast(float).alias(&#34;similarity&#34;),
                ),
                # brands_with_same_products_paired.select(
                #     pl.col(&#34;left_side&#34;),
                #     pl.col(&#34;right_side&#34;),
                #     pl.lit(1.0).alias(&#34;similarity&#34;),
                # ),
            ]
        )
        .groupby(pl.col(&#34;left_side&#34;), pl.col(&#34;right_side&#34;))
        .agg(pl.col(&#34;similarity&#34;).max())
    )

    # Load existing groups
    groups_init = pl.read_csv(f&#34;{DATA_PROCESSED_PATH}res_group_full.csv&#34;, separator=&#34;;&#34;)

    # Groups similar brands
    res_group = group_similar_strings(
        df_input_groups, groups_init=groups_init, min_similarity=0.8
    ).sort(by=&#34;group_name&#34;)

    logger.info(f&#34;Nb brands : {res_group.shape[0]},nb groups : {res_group.select(&#39;group_name&#39;).unique().shape[0]}&#34;)

    # Select a master brand
    res_group_with_master = add_master_brand(datasets, res_group)

    # Write results
    res_group_with_master.write_csv(
        f&#34;{DATA_PROCESSED_PATH}res_group_full.csv&#34;, separator=&#34;;&#34;
    )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>