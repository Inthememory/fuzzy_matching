<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fuzzy_matching.preprocessing.dataset API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fuzzy_matching.preprocessing.dataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import polars as pl
from slugify import slugify
import string
import nltk
from itertools import combinations


class Dataset:
    &#34;&#34;&#34;This class handle retailer dataset

    Args:
        df (pl.DataFrame) : dataframe that contains retailer&#39;s data
        retailer (str) : name of the retailer
        nb_levels (int) : number of levels filled
        levels_col (str) : column that contains tht list of levels
        level0_included (list) : list of level0 to include
        level1_excluded (list) : list of level1 to exclude
        replacements_brand (list) : list of replacement rules e.g. [[&#39;|&#39;, &#39;or&#39;], [&#39;%&#39;, &#39;percent&#39;]]
    &#34;&#34;&#34;

    def __init__(
        self,
        df: pl.DataFrame,
        retailer: str,
        nb_levels: int,
        levels_col: str = &#34;crumb&#34;,
        level0_included: list = [],
        level1_excluded: list = [],
        replacements_brand: list = [],
    ) -&gt; None:
        self.retailer = retailer
        self.df = df
        self.levels_col = levels_col
        self.nb_levels = nb_levels
        self.level0_included = level0_included
        self.level1_excluded = level1_excluded
        self.replacements_brand = replacements_brand
        self.dataset_cleaned = self.clean_dataset()

    def clean_dataset(self) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Proceed to a panel of transformations to clean the dataset

        Returns:
            pl.Dataframe: a polars Dataframe preprocessed
        &#34;&#34;&#34;

        # Explode levels into multiple columns
        dataset = self.expand_levels(
            self.df.select(
                pl.col(&#34;ean&#34;).alias(&#34;product_id&#34;),
                pl.col(&#34;brand_name&#34;).alias(&#34;brand_desc&#34;),
                &#34;crumb&#34;,
            ),
            self.levels_col,
            self.nb_levels,
        )

        # Filter dataset
        dataset = (
            dataset.filter(pl.col(&#34;product_id&#34;).str.contains(&#34;^[0-9]*$&#34;))
            .filter(pl.col(&#34;product_id&#34;).str.contains(&#34;[1-9]+&#34;))
            .filter(pl.col(&#34;brand_desc&#34;).is_not_null())
        )

        # Transform columns
        dataset = (
            dataset.with_columns(pl.lit(self.retailer).alias(&#34;retailer&#34;))
            .with_columns(pl.col(&#34;product_id&#34;).str.zfill(13))
            .with_columns(pl.col(&#34;brand_desc&#34;).str.to_uppercase().str.strip())
            .with_columns(
                pl.col(&#34;brand_desc&#34;)
                .apply(
                    lambda x: Dataset.upper_slug(
                        x, replacements=self.replacements_brand
                    )
                )
                .alias(&#34;brand_desc_slug&#34;)
            )
        )

        # Select columns
        dataset = dataset.select(
            [&#34;retailer&#34;, &#34;product_id&#34;, &#34;brand_desc&#34;, &#34;brand_desc_slug&#34;]
            + [
                pl.col(f&#34;level{i}&#34;).apply(lambda x: Dataset.upper_slug(x))
                for i in range(self.nb_levels)
            ]
        )
        return dataset

    @staticmethod
    def expand_levels(
        df: pl.DataFrame, levels_col: str, nb_levels: int
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Explode horizontally the column that contains the list of level into several columns

        Args:
            df (pl.DataFrame): _description_
            levels_col (str): column that contains the list of levels
            nb_levels (int): number of levels filled

        Returns:
            pl.DataFrame: a dataframe with the levels split into several columns
        &#34;&#34;&#34;
        return df.with_columns(
            [
                pl.col(levels_col).list.get(level_id).alias(f&#34;level{level_id}&#34;)
                for level_id in range(nb_levels)
            ]
        ).drop(levels_col)

    def filter_dataset(self, unknown_brands: list = []) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Remove outliers base on levels and brand

        Args:
            unknown_brands (list, optional): list of unknown brands to remove. Defaults to [].

        Returns:
            pl.Dataframe: a dataframe filtered
        &#34;&#34;&#34;
        dataset_filtered = (
            self.dataset_cleaned.filter(~pl.col(&#34;level1&#34;).is_in(self.level1_excluded))
            .filter(~pl.col(&#34;brand_desc_slug&#34;).is_in([item for item in unknown_brands]))
            .filter(pl.col(&#34;brand_desc_slug&#34;).str.contains(&#34;[a-zA-Z]+&#34;))
            .filter(pl.col(&#34;brand_desc_slug&#34;).str.contains(&#34;\w{3,}&#34;))
        )
        if len(self.level0_included) &gt; 0:
            dataset_filtered = dataset_filtered.filter(
                pl.col(&#34;level0&#34;).is_in(self.level0_included)
            )
        print(self.dataset_cleaned.shape, dataset_filtered.shape)
        return dataset_filtered

    @staticmethod
    def upper_slug(sentence: str, replacements: list = [], separator: str = &#34; &#34;) -&gt; str:
        &#34;&#34;&#34;proceed to a pannel of transformations : slugify, upper, strip

        Args:
            sentence (str): sentence to update
            replacements (list, optional): list of replacement rules e.g. [[&#39;|&#39;, &#39;or&#39;], [&#39;%&#39;, &#39;percent&#39;]]. Defaults to [].
            separator (str, optional): separator between words. Defaults to &#34; &#34;.

        Returns:
            str: sentence updated
        &#34;&#34;&#34;
        return (
            slugify(sentence, replacements=replacements, separator=separator)
            .upper()
            .strip()
        )


class DatasetsMerged:
    &#34;&#34;&#34;This class handle a list of retailers datasets

    Args:
        sdf (list) : a list that contains retailers dataframes
    &#34;&#34;&#34;

    def __init__(self, sdf: list) -&gt; None:
        self.sdf = sdf
        self.brand_classification_dummy = None
        self.brand_classification_words = None
        self.brands_updated = None

    def get_brand_classification(self, levels: list) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Create a dataframe containing brands classification

        Args:
            self
            levels (list): classification&#39;s levels to extract

        Returns:
            pl.Dataframe: a dataframe gathering all classifications
        &#34;&#34;&#34;
        return (
            pl.concat(
                [
                    dataset.select(
                        [
                            pl.col(&#34;product_id&#34;),
                            pl.col(&#34;brand_desc_slug&#34;).alias(f&#34;brand_desc_slug_{i}&#34;),
                        ]
                        + [pl.col(f&#34;level{l}&#34;).alias(f&#34;level{l}_{i}&#34;) for l in levels]
                    )
                    for i, dataset in enumerate(self.sdf)
                ],
                how=&#34;align&#34;,
            )
            .with_columns(
                pl.concat_list(
                    [f&#34;brand_desc_slug_{i}&#34; for i, _ in enumerate(self.sdf)]
                ).alias(&#34;brand_desc_slug&#34;)
            )
            .drop([f&#34;brand_desc_slug_{i}&#34; for i, _ in enumerate(self.sdf)])
            .explode(&#34;brand_desc_slug&#34;)
            .filter(pl.col(&#34;brand_desc_slug&#34;).is_not_null())
            .drop(&#34;product_id&#34;)
            .unique()
        )

    def get_brand_classification_dummy(self, levels: list) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Convert classification variable into dummy variables.

        Args:
            self
            levels (list): classification levels to convert into dummy variables

        Returns:
            pl.DataFrame: polars dataframe
        &#34;&#34;&#34;
        # Select levels
        brands_classification = self.get_brand_classification(levels)

        # Convert levels into dummy variables.
        brands_classification_dummy = (
            brands_classification.select(
                [&#34;brand_desc_slug&#34;]
                + [
                    pl.col(c)
                    for c in brands_classification.columns
                    if c.startswith(&#34;level&#34;)
                ]
            )
            .unique()
            .to_dummies(
                [c for c in brands_classification.columns if c.startswith(&#34;level&#34;)]
            )
        )

        # Agregate at brand level using maximum
        brands_classification_dummy_agregated = (
            brands_classification_dummy.drop(
                [
                    col
                    for col in brands_classification_dummy.columns
                    if col.endswith(&#34;null&#34;)
                ]
            )
            .groupby(&#34;brand_desc_slug&#34;)
            .max()
        )
        self.brand_classification_dummy = brands_classification_dummy_agregated
        return self.brand_classification_dummy

    def get_brand_classification_words(
        self, levels: list, lemmatizer, list_stopwords: list = []
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Preprocess level to make comparisons easier

        Args:
            levels (list): levels to process
            lemmatizer (type): lemmatizer
            list_stopwords (list, optional): list of stopwords. Defaults to [].

        Returns:
            pl.Dataframe: _description_
        &#34;&#34;&#34;
        ## For level in levels list levels describing each brand
        L = []
        for level in levels:
            # Select level to process
            brands_classification_level = self.get_brand_classification([level])
            brands_classification_words_level = (
                brands_classification_level
                # List levels describing each brand
                .with_columns(
                    pl.concat_list(
                        [
                            c
                            for c in brands_classification_level.columns
                            if c.startswith(&#34;level&#34;)
                        ]
                    ).alias(f&#34;level&#34;)
                )
                .select(&#34;brand_desc_slug&#34;, &#34;level&#34;)
                .explode(f&#34;level&#34;)
                .filter(pl.col(f&#34;level&#34;).is_not_null())
            )
            # Add dataframe to the list of dataframes to merge
            L.append(brands_classification_words_level)

        brands_classification_words = (
            pl.concat(L)
            .groupby(&#34;brand_desc_slug&#34;)
            .agg(pl.col(f&#34;level&#34;))
            .with_columns(pl.col(f&#34;level&#34;).cast(pl.List(pl.Utf8)).list.join(&#34; &#34;))
        )

        # Keep significant words
        brands_classification_words_updated = DatasetsMerged.update_level_col(
            brands_classification_words, lemmatizer, list_stopwords
        )
        self.brand_classification_words = brands_classification_words_updated
        return self.brand_classification_words

    def extract_brands(
        self,
        lemmatizer,
        list_stopwords: list = [],
        generic_words: list = [],
        replacements: list = [],
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Concat vertically brand_desc_slug columns.

        Args:
            lemmatizer (_type_): _description_
            list_stopwords (list, optional): _description_. Defaults to [].
            generic_words (list, optional): _description_. Defaults to [].
            replacements (list, optional): _description_. Defaults to [].

        Returns:
            pl.DataFrame: _description_
        &#34;&#34;&#34;
        # Concat vertically brand_desc_slug
        brands_df = pl.concat(
            [dataset.select(pl.col(&#34;brand_desc_slug&#34;)) for dataset in self.sdf],
            how=&#34;vertical&#34;,
        ).unique()

        # Preprocess brand name to make comparisons easier
        brands_df_updated = DatasetsMerged.update_brand_col(
            brands_df, lemmatizer, list_stopwords, generic_words, replacements
        )
        self.brands_updated = brands_df_updated
        return self.brands_updated

    def pair_brands_with_same_products(self) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Pairs two brands related two the same product.

        Args:
            datasets (list): list of dataframe containg products dans brands

        Returns:
            pl.DataFrame: a polars dataframe reporting pairs of brands
        &#34;&#34;&#34;
        # All possible pairs in List using combinations()
        pairs = list(combinations(self.sdf, 2))

        # Pairs two brands related two the same product
        datasets_paired = []
        for pair in pairs:
            datasets_paired.append(
                pl.concat(
                    [
                        pair[0].select(
                            pl.col(&#34;product_id&#34;),
                            pl.col(&#34;brand_desc_slug&#34;).alias(&#34;brand_desc_left&#34;),
                        ),
                        pair[1].select(
                            pl.col(&#34;product_id&#34;),
                            pl.col(&#34;brand_desc_slug&#34;).alias(&#34;brand_desc_right&#34;),
                        ),
                    ],
                    how=&#34;align&#34;,
                )
                .filter(pl.col(&#34;brand_desc_left&#34;).is_not_null())
                .filter(pl.col(&#34;brand_desc_right&#34;).is_not_null())
                .filter(pl.col(&#34;brand_desc_left&#34;) != pl.col(&#34;brand_desc_right&#34;))
                .groupby(&#34;brand_desc_left&#34;, &#34;brand_desc_right&#34;)
                .count()
                .filter(pl.col(&#34;count&#34;) &gt; 1)
                .select(&#34;brand_desc_left&#34;, &#34;brand_desc_right&#34;)
            )

        datasets_paired_concat = pl.concat(datasets_paired, how=&#34;vertical&#34;)

        # Invert columns
        datasets_paired_concat_invert = (
            datasets_paired_concat.with_columns(pl.col(&#34;brand_desc_left&#34;).alias(&#34;tmp&#34;))
            .with_columns(pl.col(&#34;brand_desc_right&#34;).alias(&#34;brand_desc_left&#34;))
            .with_columns(pl.col(&#34;tmp&#34;).alias(&#34;brand_desc_right&#34;))
            .select(pl.col(&#34;brand_desc_left&#34;), pl.col(&#34;brand_desc_right&#34;))
        )

        return (
            pl.concat(
                [datasets_paired_concat, datasets_paired_concat_invert], how=&#34;vertical&#34;
            )
            .unique()
            .select(
                pl.col(&#34;brand_desc_left&#34;).alias(&#34;left_side&#34;),
                pl.col(&#34;brand_desc_right&#34;).alias(&#34;right_side&#34;),
            )
        )

    @staticmethod
    def cross_join(df: pl.DataFrame, cols: list) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Create a dataframe containg all brand pairs
        Args:
            df (pl.DataFrame): dataframe input
            cols (list): columns to keep

        Returns:
            pl.DataFrame: Cartesian Product of the dataframe
        &#34;&#34;&#34;
        return df.join(
            df,
            how=&#34;cross&#34;,
        ).select(
            [pl.col(col).alias(f&#34;{col}_left&#34;) for col in cols]
            + [pl.col(f&#34;{col}_right&#34;) for col in cols]
        )

    @staticmethod
    def deduplicate_sentence(sentence: str) -&gt; str:
        sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
        sentence_deduplicated = list(
            dict.fromkeys([word for word in sentence_tokenized])
        )
        return &#34; &#34;.join(word.upper() for word in sentence_deduplicated)

    @staticmethod
    def lemmatize_sentence(sentence: str, lemmatizer) -&gt; str:
        sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
        sentence_lemmatized = [
            lemmatizer.lemmatize(word) for word in sentence_tokenized
        ]
        return &#34; &#34;.join(word.upper() for word in sentence_lemmatized)

    @staticmethod
    def remove_digit(sentence: str) -&gt; str:
        sentence_w_num = &#34;&#34;.join(i for i in sentence if not i.isdigit())
        return sentence_w_num

    @staticmethod
    def remove_generic_words(sentence: str, generic_words: list) -&gt; str:
        sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
        sentence_w_generic_words = [
            word for word in sentence_tokenized if word.lower() not in generic_words
        ]
        return &#34; &#34;.join(word.upper() for word in sentence_w_generic_words)

    @staticmethod
    def remove_punctuation(sentence: str) -&gt; str:
        sentence_w_punct = &#34;&#34;.join(
            [i.lower() for i in sentence if i not in string.punctuation]
        )
        return sentence_w_punct

    @staticmethod
    def remove_stopwords(sentence, list_stopwords: list) -&gt; str:
        sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
        sentence_w_stopwords = [
            word for word in sentence_tokenized if word.lower() not in list_stopwords
        ]
        return &#34; &#34;.join(word for word in sentence_w_stopwords)

    @staticmethod
    def update_brand_col(
        df: pl.DataFrame,
        lemmatizer,
        list_stopwords: list,
        generic_words: list,
        replacements: list,
        col: str = &#34;brand_desc_slug&#34;,
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;proceed to a pannel of transformations on a column : remove_stopwords, remove_generic_words, lemmatize_sentence, deduplicate_sentence, slugify

        Args:
            df (pl.DataFrame): _description_
            lemmatizer (_type_): lemmatizer
            list_stopwords (list): list of stopwords to remove
            generic_words (list): list of generic words to remove
            replacements (list): list of replacements to performe
            col (str, optional): col. Defaults to &#34;brand_desc_slug&#34;.

        Returns:
            pl.DataFrame: a dataframe with transformations on the sepcified column performed
        &#34;&#34;&#34;
        return (
            df.with_columns(
                pl.col(col)
                .apply(lambda x: DatasetsMerged.remove_stopwords(x, list_stopwords))
                .alias(f&#34;{col}_updated&#34;)
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.lemmatize_sentence(x, lemmatizer)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.remove_generic_words(x, generic_words)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: slugify(x, separator=&#34; &#34;, replacements=replacements)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.deduplicate_sentence(x)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;)
                .apply(lambda x: slugify(x, separator=&#34;&#34;))
                .alias(f&#34;{col}_updated_w_space&#34;)
            )
        )

    @staticmethod
    def update_level_col(
        df: pl.DataFrame, lemmatizer, list_stopwords: list, col: str = &#34;level&#34;
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;proceed to a pannel of transformations on a column : remove_stopwords, remove_digit, lemmatize_sentence, deduplicate_sentence

        Args:
            df (pl.DataFrame): _description_
            lemmatizer (_type_): lemmatizer
            list_stopwords (list): list_stopwords to remove
            col (str, optional): col. Defaults to &#34;level&#34;.

        Returns:
            pl.DataFrame: a dataframe with transformations on the sepcified column performed
        &#34;&#34;&#34;
        return (
            df.with_columns(
                pl.col(col)
                .apply(lambda x: DatasetsMerged.remove_stopwords(x, list_stopwords))
                .alias(f&#34;{col}_updated&#34;)
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(lambda x: DatasetsMerged.remove_digit(x))
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.lemmatize_sentence(x, lemmatizer)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.deduplicate_sentence(x)
                )
            )
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fuzzy_matching.preprocessing.dataset.Dataset"><code class="flex name class">
<span>class <span class="ident">Dataset</span></span>
<span>(</span><span>df: polars.dataframe.frame.DataFrame, retailer: str, nb_levels: int, levels_col: str = 'crumb', level0_included: list = [], level1_excluded: list = [], replacements_brand: list = [])</span>
</code></dt>
<dd>
<div class="desc"><p>This class handle retailer dataset</p>
<h2 id="args">Args</h2>
<p>df (pl.DataFrame) : dataframe that contains retailer's data
retailer (str) : name of the retailer
nb_levels (int) : number of levels filled
levels_col (str) : column that contains tht list of levels
level0_included (list) : list of level0 to include
level1_excluded (list) : list of level1 to exclude
replacements_brand (list) : list of replacement rules e.g. [['|', 'or'], ['%', 'percent']]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dataset:
    &#34;&#34;&#34;This class handle retailer dataset

    Args:
        df (pl.DataFrame) : dataframe that contains retailer&#39;s data
        retailer (str) : name of the retailer
        nb_levels (int) : number of levels filled
        levels_col (str) : column that contains tht list of levels
        level0_included (list) : list of level0 to include
        level1_excluded (list) : list of level1 to exclude
        replacements_brand (list) : list of replacement rules e.g. [[&#39;|&#39;, &#39;or&#39;], [&#39;%&#39;, &#39;percent&#39;]]
    &#34;&#34;&#34;

    def __init__(
        self,
        df: pl.DataFrame,
        retailer: str,
        nb_levels: int,
        levels_col: str = &#34;crumb&#34;,
        level0_included: list = [],
        level1_excluded: list = [],
        replacements_brand: list = [],
    ) -&gt; None:
        self.retailer = retailer
        self.df = df
        self.levels_col = levels_col
        self.nb_levels = nb_levels
        self.level0_included = level0_included
        self.level1_excluded = level1_excluded
        self.replacements_brand = replacements_brand
        self.dataset_cleaned = self.clean_dataset()

    def clean_dataset(self) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Proceed to a panel of transformations to clean the dataset

        Returns:
            pl.Dataframe: a polars Dataframe preprocessed
        &#34;&#34;&#34;

        # Explode levels into multiple columns
        dataset = self.expand_levels(
            self.df.select(
                pl.col(&#34;ean&#34;).alias(&#34;product_id&#34;),
                pl.col(&#34;brand_name&#34;).alias(&#34;brand_desc&#34;),
                &#34;crumb&#34;,
            ),
            self.levels_col,
            self.nb_levels,
        )

        # Filter dataset
        dataset = (
            dataset.filter(pl.col(&#34;product_id&#34;).str.contains(&#34;^[0-9]*$&#34;))
            .filter(pl.col(&#34;product_id&#34;).str.contains(&#34;[1-9]+&#34;))
            .filter(pl.col(&#34;brand_desc&#34;).is_not_null())
        )

        # Transform columns
        dataset = (
            dataset.with_columns(pl.lit(self.retailer).alias(&#34;retailer&#34;))
            .with_columns(pl.col(&#34;product_id&#34;).str.zfill(13))
            .with_columns(pl.col(&#34;brand_desc&#34;).str.to_uppercase().str.strip())
            .with_columns(
                pl.col(&#34;brand_desc&#34;)
                .apply(
                    lambda x: Dataset.upper_slug(
                        x, replacements=self.replacements_brand
                    )
                )
                .alias(&#34;brand_desc_slug&#34;)
            )
        )

        # Select columns
        dataset = dataset.select(
            [&#34;retailer&#34;, &#34;product_id&#34;, &#34;brand_desc&#34;, &#34;brand_desc_slug&#34;]
            + [
                pl.col(f&#34;level{i}&#34;).apply(lambda x: Dataset.upper_slug(x))
                for i in range(self.nb_levels)
            ]
        )
        return dataset

    @staticmethod
    def expand_levels(
        df: pl.DataFrame, levels_col: str, nb_levels: int
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Explode horizontally the column that contains the list of level into several columns

        Args:
            df (pl.DataFrame): _description_
            levels_col (str): column that contains the list of levels
            nb_levels (int): number of levels filled

        Returns:
            pl.DataFrame: a dataframe with the levels split into several columns
        &#34;&#34;&#34;
        return df.with_columns(
            [
                pl.col(levels_col).list.get(level_id).alias(f&#34;level{level_id}&#34;)
                for level_id in range(nb_levels)
            ]
        ).drop(levels_col)

    def filter_dataset(self, unknown_brands: list = []) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Remove outliers base on levels and brand

        Args:
            unknown_brands (list, optional): list of unknown brands to remove. Defaults to [].

        Returns:
            pl.Dataframe: a dataframe filtered
        &#34;&#34;&#34;
        dataset_filtered = (
            self.dataset_cleaned.filter(~pl.col(&#34;level1&#34;).is_in(self.level1_excluded))
            .filter(~pl.col(&#34;brand_desc_slug&#34;).is_in([item for item in unknown_brands]))
            .filter(pl.col(&#34;brand_desc_slug&#34;).str.contains(&#34;[a-zA-Z]+&#34;))
            .filter(pl.col(&#34;brand_desc_slug&#34;).str.contains(&#34;\w{3,}&#34;))
        )
        if len(self.level0_included) &gt; 0:
            dataset_filtered = dataset_filtered.filter(
                pl.col(&#34;level0&#34;).is_in(self.level0_included)
            )
        print(self.dataset_cleaned.shape, dataset_filtered.shape)
        return dataset_filtered

    @staticmethod
    def upper_slug(sentence: str, replacements: list = [], separator: str = &#34; &#34;) -&gt; str:
        &#34;&#34;&#34;proceed to a pannel of transformations : slugify, upper, strip

        Args:
            sentence (str): sentence to update
            replacements (list, optional): list of replacement rules e.g. [[&#39;|&#39;, &#39;or&#39;], [&#39;%&#39;, &#39;percent&#39;]]. Defaults to [].
            separator (str, optional): separator between words. Defaults to &#34; &#34;.

        Returns:
            str: sentence updated
        &#34;&#34;&#34;
        return (
            slugify(sentence, replacements=replacements, separator=separator)
            .upper()
            .strip()
        )</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="fuzzy_matching.preprocessing.dataset.Dataset.expand_levels"><code class="name flex">
<span>def <span class="ident">expand_levels</span></span>(<span>df: polars.dataframe.frame.DataFrame, levels_col: str, nb_levels: int) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Explode horizontally the column that contains the list of level into several columns</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pl.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>levels_col</code></strong> :&ensp;<code>str</code></dt>
<dd>column that contains the list of levels</dd>
<dt><strong><code>nb_levels</code></strong> :&ensp;<code>int</code></dt>
<dd>number of levels filled</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.DataFrame</code></dt>
<dd>a dataframe with the levels split into several columns</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def expand_levels(
    df: pl.DataFrame, levels_col: str, nb_levels: int
) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Explode horizontally the column that contains the list of level into several columns

    Args:
        df (pl.DataFrame): _description_
        levels_col (str): column that contains the list of levels
        nb_levels (int): number of levels filled

    Returns:
        pl.DataFrame: a dataframe with the levels split into several columns
    &#34;&#34;&#34;
    return df.with_columns(
        [
            pl.col(levels_col).list.get(level_id).alias(f&#34;level{level_id}&#34;)
            for level_id in range(nb_levels)
        ]
    ).drop(levels_col)</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.Dataset.upper_slug"><code class="name flex">
<span>def <span class="ident">upper_slug</span></span>(<span>sentence: str, replacements: list = [], separator: str = ' ') ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>proceed to a pannel of transformations : slugify, upper, strip</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sentence</code></strong> :&ensp;<code>str</code></dt>
<dd>sentence to update</dd>
<dt><strong><code>replacements</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>list of replacement rules e.g. [['|', 'or'], ['%', 'percent']]. Defaults to [].</dd>
<dt><strong><code>separator</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>separator between words. Defaults to " ".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>sentence updated</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def upper_slug(sentence: str, replacements: list = [], separator: str = &#34; &#34;) -&gt; str:
    &#34;&#34;&#34;proceed to a pannel of transformations : slugify, upper, strip

    Args:
        sentence (str): sentence to update
        replacements (list, optional): list of replacement rules e.g. [[&#39;|&#39;, &#39;or&#39;], [&#39;%&#39;, &#39;percent&#39;]]. Defaults to [].
        separator (str, optional): separator between words. Defaults to &#34; &#34;.

    Returns:
        str: sentence updated
    &#34;&#34;&#34;
    return (
        slugify(sentence, replacements=replacements, separator=separator)
        .upper()
        .strip()
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fuzzy_matching.preprocessing.dataset.Dataset.clean_dataset"><code class="name flex">
<span>def <span class="ident">clean_dataset</span></span>(<span>self) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Proceed to a panel of transformations to clean the dataset</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.Dataframe</code></dt>
<dd>a polars Dataframe preprocessed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_dataset(self) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Proceed to a panel of transformations to clean the dataset

    Returns:
        pl.Dataframe: a polars Dataframe preprocessed
    &#34;&#34;&#34;

    # Explode levels into multiple columns
    dataset = self.expand_levels(
        self.df.select(
            pl.col(&#34;ean&#34;).alias(&#34;product_id&#34;),
            pl.col(&#34;brand_name&#34;).alias(&#34;brand_desc&#34;),
            &#34;crumb&#34;,
        ),
        self.levels_col,
        self.nb_levels,
    )

    # Filter dataset
    dataset = (
        dataset.filter(pl.col(&#34;product_id&#34;).str.contains(&#34;^[0-9]*$&#34;))
        .filter(pl.col(&#34;product_id&#34;).str.contains(&#34;[1-9]+&#34;))
        .filter(pl.col(&#34;brand_desc&#34;).is_not_null())
    )

    # Transform columns
    dataset = (
        dataset.with_columns(pl.lit(self.retailer).alias(&#34;retailer&#34;))
        .with_columns(pl.col(&#34;product_id&#34;).str.zfill(13))
        .with_columns(pl.col(&#34;brand_desc&#34;).str.to_uppercase().str.strip())
        .with_columns(
            pl.col(&#34;brand_desc&#34;)
            .apply(
                lambda x: Dataset.upper_slug(
                    x, replacements=self.replacements_brand
                )
            )
            .alias(&#34;brand_desc_slug&#34;)
        )
    )

    # Select columns
    dataset = dataset.select(
        [&#34;retailer&#34;, &#34;product_id&#34;, &#34;brand_desc&#34;, &#34;brand_desc_slug&#34;]
        + [
            pl.col(f&#34;level{i}&#34;).apply(lambda x: Dataset.upper_slug(x))
            for i in range(self.nb_levels)
        ]
    )
    return dataset</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.Dataset.filter_dataset"><code class="name flex">
<span>def <span class="ident">filter_dataset</span></span>(<span>self, unknown_brands: list = []) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Remove outliers base on levels and brand</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>unknown_brands</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>list of unknown brands to remove. Defaults to [].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.Dataframe</code></dt>
<dd>a dataframe filtered</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_dataset(self, unknown_brands: list = []) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Remove outliers base on levels and brand

    Args:
        unknown_brands (list, optional): list of unknown brands to remove. Defaults to [].

    Returns:
        pl.Dataframe: a dataframe filtered
    &#34;&#34;&#34;
    dataset_filtered = (
        self.dataset_cleaned.filter(~pl.col(&#34;level1&#34;).is_in(self.level1_excluded))
        .filter(~pl.col(&#34;brand_desc_slug&#34;).is_in([item for item in unknown_brands]))
        .filter(pl.col(&#34;brand_desc_slug&#34;).str.contains(&#34;[a-zA-Z]+&#34;))
        .filter(pl.col(&#34;brand_desc_slug&#34;).str.contains(&#34;\w{3,}&#34;))
    )
    if len(self.level0_included) &gt; 0:
        dataset_filtered = dataset_filtered.filter(
            pl.col(&#34;level0&#34;).is_in(self.level0_included)
        )
    print(self.dataset_cleaned.shape, dataset_filtered.shape)
    return dataset_filtered</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged"><code class="flex name class">
<span>class <span class="ident">DatasetsMerged</span></span>
<span>(</span><span>sdf: list)</span>
</code></dt>
<dd>
<div class="desc"><p>This class handle a list of retailers datasets</p>
<h2 id="args">Args</h2>
<p>sdf (list) : a list that contains retailers dataframes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DatasetsMerged:
    &#34;&#34;&#34;This class handle a list of retailers datasets

    Args:
        sdf (list) : a list that contains retailers dataframes
    &#34;&#34;&#34;

    def __init__(self, sdf: list) -&gt; None:
        self.sdf = sdf
        self.brand_classification_dummy = None
        self.brand_classification_words = None
        self.brands_updated = None

    def get_brand_classification(self, levels: list) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Create a dataframe containing brands classification

        Args:
            self
            levels (list): classification&#39;s levels to extract

        Returns:
            pl.Dataframe: a dataframe gathering all classifications
        &#34;&#34;&#34;
        return (
            pl.concat(
                [
                    dataset.select(
                        [
                            pl.col(&#34;product_id&#34;),
                            pl.col(&#34;brand_desc_slug&#34;).alias(f&#34;brand_desc_slug_{i}&#34;),
                        ]
                        + [pl.col(f&#34;level{l}&#34;).alias(f&#34;level{l}_{i}&#34;) for l in levels]
                    )
                    for i, dataset in enumerate(self.sdf)
                ],
                how=&#34;align&#34;,
            )
            .with_columns(
                pl.concat_list(
                    [f&#34;brand_desc_slug_{i}&#34; for i, _ in enumerate(self.sdf)]
                ).alias(&#34;brand_desc_slug&#34;)
            )
            .drop([f&#34;brand_desc_slug_{i}&#34; for i, _ in enumerate(self.sdf)])
            .explode(&#34;brand_desc_slug&#34;)
            .filter(pl.col(&#34;brand_desc_slug&#34;).is_not_null())
            .drop(&#34;product_id&#34;)
            .unique()
        )

    def get_brand_classification_dummy(self, levels: list) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Convert classification variable into dummy variables.

        Args:
            self
            levels (list): classification levels to convert into dummy variables

        Returns:
            pl.DataFrame: polars dataframe
        &#34;&#34;&#34;
        # Select levels
        brands_classification = self.get_brand_classification(levels)

        # Convert levels into dummy variables.
        brands_classification_dummy = (
            brands_classification.select(
                [&#34;brand_desc_slug&#34;]
                + [
                    pl.col(c)
                    for c in brands_classification.columns
                    if c.startswith(&#34;level&#34;)
                ]
            )
            .unique()
            .to_dummies(
                [c for c in brands_classification.columns if c.startswith(&#34;level&#34;)]
            )
        )

        # Agregate at brand level using maximum
        brands_classification_dummy_agregated = (
            brands_classification_dummy.drop(
                [
                    col
                    for col in brands_classification_dummy.columns
                    if col.endswith(&#34;null&#34;)
                ]
            )
            .groupby(&#34;brand_desc_slug&#34;)
            .max()
        )
        self.brand_classification_dummy = brands_classification_dummy_agregated
        return self.brand_classification_dummy

    def get_brand_classification_words(
        self, levels: list, lemmatizer, list_stopwords: list = []
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Preprocess level to make comparisons easier

        Args:
            levels (list): levels to process
            lemmatizer (type): lemmatizer
            list_stopwords (list, optional): list of stopwords. Defaults to [].

        Returns:
            pl.Dataframe: _description_
        &#34;&#34;&#34;
        ## For level in levels list levels describing each brand
        L = []
        for level in levels:
            # Select level to process
            brands_classification_level = self.get_brand_classification([level])
            brands_classification_words_level = (
                brands_classification_level
                # List levels describing each brand
                .with_columns(
                    pl.concat_list(
                        [
                            c
                            for c in brands_classification_level.columns
                            if c.startswith(&#34;level&#34;)
                        ]
                    ).alias(f&#34;level&#34;)
                )
                .select(&#34;brand_desc_slug&#34;, &#34;level&#34;)
                .explode(f&#34;level&#34;)
                .filter(pl.col(f&#34;level&#34;).is_not_null())
            )
            # Add dataframe to the list of dataframes to merge
            L.append(brands_classification_words_level)

        brands_classification_words = (
            pl.concat(L)
            .groupby(&#34;brand_desc_slug&#34;)
            .agg(pl.col(f&#34;level&#34;))
            .with_columns(pl.col(f&#34;level&#34;).cast(pl.List(pl.Utf8)).list.join(&#34; &#34;))
        )

        # Keep significant words
        brands_classification_words_updated = DatasetsMerged.update_level_col(
            brands_classification_words, lemmatizer, list_stopwords
        )
        self.brand_classification_words = brands_classification_words_updated
        return self.brand_classification_words

    def extract_brands(
        self,
        lemmatizer,
        list_stopwords: list = [],
        generic_words: list = [],
        replacements: list = [],
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Concat vertically brand_desc_slug columns.

        Args:
            lemmatizer (_type_): _description_
            list_stopwords (list, optional): _description_. Defaults to [].
            generic_words (list, optional): _description_. Defaults to [].
            replacements (list, optional): _description_. Defaults to [].

        Returns:
            pl.DataFrame: _description_
        &#34;&#34;&#34;
        # Concat vertically brand_desc_slug
        brands_df = pl.concat(
            [dataset.select(pl.col(&#34;brand_desc_slug&#34;)) for dataset in self.sdf],
            how=&#34;vertical&#34;,
        ).unique()

        # Preprocess brand name to make comparisons easier
        brands_df_updated = DatasetsMerged.update_brand_col(
            brands_df, lemmatizer, list_stopwords, generic_words, replacements
        )
        self.brands_updated = brands_df_updated
        return self.brands_updated

    def pair_brands_with_same_products(self) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Pairs two brands related two the same product.

        Args:
            datasets (list): list of dataframe containg products dans brands

        Returns:
            pl.DataFrame: a polars dataframe reporting pairs of brands
        &#34;&#34;&#34;
        # All possible pairs in List using combinations()
        pairs = list(combinations(self.sdf, 2))

        # Pairs two brands related two the same product
        datasets_paired = []
        for pair in pairs:
            datasets_paired.append(
                pl.concat(
                    [
                        pair[0].select(
                            pl.col(&#34;product_id&#34;),
                            pl.col(&#34;brand_desc_slug&#34;).alias(&#34;brand_desc_left&#34;),
                        ),
                        pair[1].select(
                            pl.col(&#34;product_id&#34;),
                            pl.col(&#34;brand_desc_slug&#34;).alias(&#34;brand_desc_right&#34;),
                        ),
                    ],
                    how=&#34;align&#34;,
                )
                .filter(pl.col(&#34;brand_desc_left&#34;).is_not_null())
                .filter(pl.col(&#34;brand_desc_right&#34;).is_not_null())
                .filter(pl.col(&#34;brand_desc_left&#34;) != pl.col(&#34;brand_desc_right&#34;))
                .groupby(&#34;brand_desc_left&#34;, &#34;brand_desc_right&#34;)
                .count()
                .filter(pl.col(&#34;count&#34;) &gt; 1)
                .select(&#34;brand_desc_left&#34;, &#34;brand_desc_right&#34;)
            )

        datasets_paired_concat = pl.concat(datasets_paired, how=&#34;vertical&#34;)

        # Invert columns
        datasets_paired_concat_invert = (
            datasets_paired_concat.with_columns(pl.col(&#34;brand_desc_left&#34;).alias(&#34;tmp&#34;))
            .with_columns(pl.col(&#34;brand_desc_right&#34;).alias(&#34;brand_desc_left&#34;))
            .with_columns(pl.col(&#34;tmp&#34;).alias(&#34;brand_desc_right&#34;))
            .select(pl.col(&#34;brand_desc_left&#34;), pl.col(&#34;brand_desc_right&#34;))
        )

        return (
            pl.concat(
                [datasets_paired_concat, datasets_paired_concat_invert], how=&#34;vertical&#34;
            )
            .unique()
            .select(
                pl.col(&#34;brand_desc_left&#34;).alias(&#34;left_side&#34;),
                pl.col(&#34;brand_desc_right&#34;).alias(&#34;right_side&#34;),
            )
        )

    @staticmethod
    def cross_join(df: pl.DataFrame, cols: list) -&gt; pl.DataFrame:
        &#34;&#34;&#34;Create a dataframe containg all brand pairs
        Args:
            df (pl.DataFrame): dataframe input
            cols (list): columns to keep

        Returns:
            pl.DataFrame: Cartesian Product of the dataframe
        &#34;&#34;&#34;
        return df.join(
            df,
            how=&#34;cross&#34;,
        ).select(
            [pl.col(col).alias(f&#34;{col}_left&#34;) for col in cols]
            + [pl.col(f&#34;{col}_right&#34;) for col in cols]
        )

    @staticmethod
    def deduplicate_sentence(sentence: str) -&gt; str:
        sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
        sentence_deduplicated = list(
            dict.fromkeys([word for word in sentence_tokenized])
        )
        return &#34; &#34;.join(word.upper() for word in sentence_deduplicated)

    @staticmethod
    def lemmatize_sentence(sentence: str, lemmatizer) -&gt; str:
        sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
        sentence_lemmatized = [
            lemmatizer.lemmatize(word) for word in sentence_tokenized
        ]
        return &#34; &#34;.join(word.upper() for word in sentence_lemmatized)

    @staticmethod
    def remove_digit(sentence: str) -&gt; str:
        sentence_w_num = &#34;&#34;.join(i for i in sentence if not i.isdigit())
        return sentence_w_num

    @staticmethod
    def remove_generic_words(sentence: str, generic_words: list) -&gt; str:
        sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
        sentence_w_generic_words = [
            word for word in sentence_tokenized if word.lower() not in generic_words
        ]
        return &#34; &#34;.join(word.upper() for word in sentence_w_generic_words)

    @staticmethod
    def remove_punctuation(sentence: str) -&gt; str:
        sentence_w_punct = &#34;&#34;.join(
            [i.lower() for i in sentence if i not in string.punctuation]
        )
        return sentence_w_punct

    @staticmethod
    def remove_stopwords(sentence, list_stopwords: list) -&gt; str:
        sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
        sentence_w_stopwords = [
            word for word in sentence_tokenized if word.lower() not in list_stopwords
        ]
        return &#34; &#34;.join(word for word in sentence_w_stopwords)

    @staticmethod
    def update_brand_col(
        df: pl.DataFrame,
        lemmatizer,
        list_stopwords: list,
        generic_words: list,
        replacements: list,
        col: str = &#34;brand_desc_slug&#34;,
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;proceed to a pannel of transformations on a column : remove_stopwords, remove_generic_words, lemmatize_sentence, deduplicate_sentence, slugify

        Args:
            df (pl.DataFrame): _description_
            lemmatizer (_type_): lemmatizer
            list_stopwords (list): list of stopwords to remove
            generic_words (list): list of generic words to remove
            replacements (list): list of replacements to performe
            col (str, optional): col. Defaults to &#34;brand_desc_slug&#34;.

        Returns:
            pl.DataFrame: a dataframe with transformations on the sepcified column performed
        &#34;&#34;&#34;
        return (
            df.with_columns(
                pl.col(col)
                .apply(lambda x: DatasetsMerged.remove_stopwords(x, list_stopwords))
                .alias(f&#34;{col}_updated&#34;)
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.lemmatize_sentence(x, lemmatizer)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.remove_generic_words(x, generic_words)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: slugify(x, separator=&#34; &#34;, replacements=replacements)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.deduplicate_sentence(x)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;)
                .apply(lambda x: slugify(x, separator=&#34;&#34;))
                .alias(f&#34;{col}_updated_w_space&#34;)
            )
        )

    @staticmethod
    def update_level_col(
        df: pl.DataFrame, lemmatizer, list_stopwords: list, col: str = &#34;level&#34;
    ) -&gt; pl.DataFrame:
        &#34;&#34;&#34;proceed to a pannel of transformations on a column : remove_stopwords, remove_digit, lemmatize_sentence, deduplicate_sentence

        Args:
            df (pl.DataFrame): _description_
            lemmatizer (_type_): lemmatizer
            list_stopwords (list): list_stopwords to remove
            col (str, optional): col. Defaults to &#34;level&#34;.

        Returns:
            pl.DataFrame: a dataframe with transformations on the sepcified column performed
        &#34;&#34;&#34;
        return (
            df.with_columns(
                pl.col(col)
                .apply(lambda x: DatasetsMerged.remove_stopwords(x, list_stopwords))
                .alias(f&#34;{col}_updated&#34;)
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(lambda x: DatasetsMerged.remove_digit(x))
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.lemmatize_sentence(x, lemmatizer)
                )
            )
            .with_columns(
                pl.col(f&#34;{col}_updated&#34;).apply(
                    lambda x: DatasetsMerged.deduplicate_sentence(x)
                )
            )
        )</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.cross_join"><code class="name flex">
<span>def <span class="ident">cross_join</span></span>(<span>df: polars.dataframe.frame.DataFrame, cols: list) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Create a dataframe containg all brand pairs</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pl.DataFrame</code></dt>
<dd>dataframe input</dd>
<dt><strong><code>cols</code></strong> :&ensp;<code>list</code></dt>
<dd>columns to keep</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.DataFrame</code></dt>
<dd>Cartesian Product of the dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def cross_join(df: pl.DataFrame, cols: list) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Create a dataframe containg all brand pairs
    Args:
        df (pl.DataFrame): dataframe input
        cols (list): columns to keep

    Returns:
        pl.DataFrame: Cartesian Product of the dataframe
    &#34;&#34;&#34;
    return df.join(
        df,
        how=&#34;cross&#34;,
    ).select(
        [pl.col(col).alias(f&#34;{col}_left&#34;) for col in cols]
        + [pl.col(f&#34;{col}_right&#34;) for col in cols]
    )</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.deduplicate_sentence"><code class="name flex">
<span>def <span class="ident">deduplicate_sentence</span></span>(<span>sentence: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def deduplicate_sentence(sentence: str) -&gt; str:
    sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
    sentence_deduplicated = list(
        dict.fromkeys([word for word in sentence_tokenized])
    )
    return &#34; &#34;.join(word.upper() for word in sentence_deduplicated)</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.lemmatize_sentence"><code class="name flex">
<span>def <span class="ident">lemmatize_sentence</span></span>(<span>sentence: str, lemmatizer) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def lemmatize_sentence(sentence: str, lemmatizer) -&gt; str:
    sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
    sentence_lemmatized = [
        lemmatizer.lemmatize(word) for word in sentence_tokenized
    ]
    return &#34; &#34;.join(word.upper() for word in sentence_lemmatized)</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_digit"><code class="name flex">
<span>def <span class="ident">remove_digit</span></span>(<span>sentence: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def remove_digit(sentence: str) -&gt; str:
    sentence_w_num = &#34;&#34;.join(i for i in sentence if not i.isdigit())
    return sentence_w_num</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_generic_words"><code class="name flex">
<span>def <span class="ident">remove_generic_words</span></span>(<span>sentence: str, generic_words: list) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def remove_generic_words(sentence: str, generic_words: list) -&gt; str:
    sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
    sentence_w_generic_words = [
        word for word in sentence_tokenized if word.lower() not in generic_words
    ]
    return &#34; &#34;.join(word.upper() for word in sentence_w_generic_words)</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_punctuation"><code class="name flex">
<span>def <span class="ident">remove_punctuation</span></span>(<span>sentence: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def remove_punctuation(sentence: str) -&gt; str:
    sentence_w_punct = &#34;&#34;.join(
        [i.lower() for i in sentence if i not in string.punctuation]
    )
    return sentence_w_punct</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_stopwords"><code class="name flex">
<span>def <span class="ident">remove_stopwords</span></span>(<span>sentence, list_stopwords: list) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def remove_stopwords(sentence, list_stopwords: list) -&gt; str:
    sentence_tokenized = nltk.tokenize.word_tokenize(sentence)
    sentence_w_stopwords = [
        word for word in sentence_tokenized if word.lower() not in list_stopwords
    ]
    return &#34; &#34;.join(word for word in sentence_w_stopwords)</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.update_brand_col"><code class="name flex">
<span>def <span class="ident">update_brand_col</span></span>(<span>df: polars.dataframe.frame.DataFrame, lemmatizer, list_stopwords: list, generic_words: list, replacements: list, col: str = 'brand_desc_slug') ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>proceed to a pannel of transformations on a column : remove_stopwords, remove_generic_words, lemmatize_sentence, deduplicate_sentence, slugify</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pl.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>lemmatizer</code></strong> :&ensp;<code>_type_</code></dt>
<dd>lemmatizer</dd>
<dt><strong><code>list_stopwords</code></strong> :&ensp;<code>list</code></dt>
<dd>list of stopwords to remove</dd>
<dt><strong><code>generic_words</code></strong> :&ensp;<code>list</code></dt>
<dd>list of generic words to remove</dd>
<dt><strong><code>replacements</code></strong> :&ensp;<code>list</code></dt>
<dd>list of replacements to performe</dd>
<dt><strong><code>col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>col. Defaults to "brand_desc_slug".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.DataFrame</code></dt>
<dd>a dataframe with transformations on the sepcified column performed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def update_brand_col(
    df: pl.DataFrame,
    lemmatizer,
    list_stopwords: list,
    generic_words: list,
    replacements: list,
    col: str = &#34;brand_desc_slug&#34;,
) -&gt; pl.DataFrame:
    &#34;&#34;&#34;proceed to a pannel of transformations on a column : remove_stopwords, remove_generic_words, lemmatize_sentence, deduplicate_sentence, slugify

    Args:
        df (pl.DataFrame): _description_
        lemmatizer (_type_): lemmatizer
        list_stopwords (list): list of stopwords to remove
        generic_words (list): list of generic words to remove
        replacements (list): list of replacements to performe
        col (str, optional): col. Defaults to &#34;brand_desc_slug&#34;.

    Returns:
        pl.DataFrame: a dataframe with transformations on the sepcified column performed
    &#34;&#34;&#34;
    return (
        df.with_columns(
            pl.col(col)
            .apply(lambda x: DatasetsMerged.remove_stopwords(x, list_stopwords))
            .alias(f&#34;{col}_updated&#34;)
        )
        .with_columns(
            pl.col(f&#34;{col}_updated&#34;).apply(
                lambda x: DatasetsMerged.lemmatize_sentence(x, lemmatizer)
            )
        )
        .with_columns(
            pl.col(f&#34;{col}_updated&#34;).apply(
                lambda x: DatasetsMerged.remove_generic_words(x, generic_words)
            )
        )
        .with_columns(
            pl.col(f&#34;{col}_updated&#34;).apply(
                lambda x: slugify(x, separator=&#34; &#34;, replacements=replacements)
            )
        )
        .with_columns(
            pl.col(f&#34;{col}_updated&#34;).apply(
                lambda x: DatasetsMerged.deduplicate_sentence(x)
            )
        )
        .with_columns(
            pl.col(f&#34;{col}_updated&#34;)
            .apply(lambda x: slugify(x, separator=&#34;&#34;))
            .alias(f&#34;{col}_updated_w_space&#34;)
        )
    )</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.update_level_col"><code class="name flex">
<span>def <span class="ident">update_level_col</span></span>(<span>df: polars.dataframe.frame.DataFrame, lemmatizer, list_stopwords: list, col: str = 'level') ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>proceed to a pannel of transformations on a column : remove_stopwords, remove_digit, lemmatize_sentence, deduplicate_sentence</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pl.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>lemmatizer</code></strong> :&ensp;<code>_type_</code></dt>
<dd>lemmatizer</dd>
<dt><strong><code>list_stopwords</code></strong> :&ensp;<code>list</code></dt>
<dd>list_stopwords to remove</dd>
<dt><strong><code>col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>col. Defaults to "level".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.DataFrame</code></dt>
<dd>a dataframe with transformations on the sepcified column performed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def update_level_col(
    df: pl.DataFrame, lemmatizer, list_stopwords: list, col: str = &#34;level&#34;
) -&gt; pl.DataFrame:
    &#34;&#34;&#34;proceed to a pannel of transformations on a column : remove_stopwords, remove_digit, lemmatize_sentence, deduplicate_sentence

    Args:
        df (pl.DataFrame): _description_
        lemmatizer (_type_): lemmatizer
        list_stopwords (list): list_stopwords to remove
        col (str, optional): col. Defaults to &#34;level&#34;.

    Returns:
        pl.DataFrame: a dataframe with transformations on the sepcified column performed
    &#34;&#34;&#34;
    return (
        df.with_columns(
            pl.col(col)
            .apply(lambda x: DatasetsMerged.remove_stopwords(x, list_stopwords))
            .alias(f&#34;{col}_updated&#34;)
        )
        .with_columns(
            pl.col(f&#34;{col}_updated&#34;).apply(lambda x: DatasetsMerged.remove_digit(x))
        )
        .with_columns(
            pl.col(f&#34;{col}_updated&#34;).apply(
                lambda x: DatasetsMerged.lemmatize_sentence(x, lemmatizer)
            )
        )
        .with_columns(
            pl.col(f&#34;{col}_updated&#34;).apply(
                lambda x: DatasetsMerged.deduplicate_sentence(x)
            )
        )
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.extract_brands"><code class="name flex">
<span>def <span class="ident">extract_brands</span></span>(<span>self, lemmatizer, list_stopwords: list = [], generic_words: list = [], replacements: list = []) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Concat vertically brand_desc_slug columns.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>lemmatizer</code></strong> :&ensp;<code>_type_</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>list_stopwords</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd><em>description</em>. Defaults to [].</dd>
<dt><strong><code>generic_words</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd><em>description</em>. Defaults to [].</dd>
<dt><strong><code>replacements</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd><em>description</em>. Defaults to [].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.DataFrame</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_brands(
    self,
    lemmatizer,
    list_stopwords: list = [],
    generic_words: list = [],
    replacements: list = [],
) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Concat vertically brand_desc_slug columns.

    Args:
        lemmatizer (_type_): _description_
        list_stopwords (list, optional): _description_. Defaults to [].
        generic_words (list, optional): _description_. Defaults to [].
        replacements (list, optional): _description_. Defaults to [].

    Returns:
        pl.DataFrame: _description_
    &#34;&#34;&#34;
    # Concat vertically brand_desc_slug
    brands_df = pl.concat(
        [dataset.select(pl.col(&#34;brand_desc_slug&#34;)) for dataset in self.sdf],
        how=&#34;vertical&#34;,
    ).unique()

    # Preprocess brand name to make comparisons easier
    brands_df_updated = DatasetsMerged.update_brand_col(
        brands_df, lemmatizer, list_stopwords, generic_words, replacements
    )
    self.brands_updated = brands_df_updated
    return self.brands_updated</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification"><code class="name flex">
<span>def <span class="ident">get_brand_classification</span></span>(<span>self, levels: list) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Create a dataframe containing brands classification</p>
<h2 id="args">Args</h2>
<dl>
<dt>self</dt>
<dt><strong><code>levels</code></strong> :&ensp;<code>list</code></dt>
<dd>classification's levels to extract</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.Dataframe</code></dt>
<dd>a dataframe gathering all classifications</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_brand_classification(self, levels: list) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Create a dataframe containing brands classification

    Args:
        self
        levels (list): classification&#39;s levels to extract

    Returns:
        pl.Dataframe: a dataframe gathering all classifications
    &#34;&#34;&#34;
    return (
        pl.concat(
            [
                dataset.select(
                    [
                        pl.col(&#34;product_id&#34;),
                        pl.col(&#34;brand_desc_slug&#34;).alias(f&#34;brand_desc_slug_{i}&#34;),
                    ]
                    + [pl.col(f&#34;level{l}&#34;).alias(f&#34;level{l}_{i}&#34;) for l in levels]
                )
                for i, dataset in enumerate(self.sdf)
            ],
            how=&#34;align&#34;,
        )
        .with_columns(
            pl.concat_list(
                [f&#34;brand_desc_slug_{i}&#34; for i, _ in enumerate(self.sdf)]
            ).alias(&#34;brand_desc_slug&#34;)
        )
        .drop([f&#34;brand_desc_slug_{i}&#34; for i, _ in enumerate(self.sdf)])
        .explode(&#34;brand_desc_slug&#34;)
        .filter(pl.col(&#34;brand_desc_slug&#34;).is_not_null())
        .drop(&#34;product_id&#34;)
        .unique()
    )</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification_dummy"><code class="name flex">
<span>def <span class="ident">get_brand_classification_dummy</span></span>(<span>self, levels: list) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Convert classification variable into dummy variables.</p>
<h2 id="args">Args</h2>
<dl>
<dt>self</dt>
<dt><strong><code>levels</code></strong> :&ensp;<code>list</code></dt>
<dd>classification levels to convert into dummy variables</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.DataFrame</code></dt>
<dd>polars dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_brand_classification_dummy(self, levels: list) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Convert classification variable into dummy variables.

    Args:
        self
        levels (list): classification levels to convert into dummy variables

    Returns:
        pl.DataFrame: polars dataframe
    &#34;&#34;&#34;
    # Select levels
    brands_classification = self.get_brand_classification(levels)

    # Convert levels into dummy variables.
    brands_classification_dummy = (
        brands_classification.select(
            [&#34;brand_desc_slug&#34;]
            + [
                pl.col(c)
                for c in brands_classification.columns
                if c.startswith(&#34;level&#34;)
            ]
        )
        .unique()
        .to_dummies(
            [c for c in brands_classification.columns if c.startswith(&#34;level&#34;)]
        )
    )

    # Agregate at brand level using maximum
    brands_classification_dummy_agregated = (
        brands_classification_dummy.drop(
            [
                col
                for col in brands_classification_dummy.columns
                if col.endswith(&#34;null&#34;)
            ]
        )
        .groupby(&#34;brand_desc_slug&#34;)
        .max()
    )
    self.brand_classification_dummy = brands_classification_dummy_agregated
    return self.brand_classification_dummy</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification_words"><code class="name flex">
<span>def <span class="ident">get_brand_classification_words</span></span>(<span>self, levels: list, lemmatizer, list_stopwords: list = []) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocess level to make comparisons easier</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>levels</code></strong> :&ensp;<code>list</code></dt>
<dd>levels to process</dd>
<dt><strong><code>lemmatizer</code></strong> :&ensp;<code>type</code></dt>
<dd>lemmatizer</dd>
<dt><strong><code>list_stopwords</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>list of stopwords. Defaults to [].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.Dataframe</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_brand_classification_words(
    self, levels: list, lemmatizer, list_stopwords: list = []
) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Preprocess level to make comparisons easier

    Args:
        levels (list): levels to process
        lemmatizer (type): lemmatizer
        list_stopwords (list, optional): list of stopwords. Defaults to [].

    Returns:
        pl.Dataframe: _description_
    &#34;&#34;&#34;
    ## For level in levels list levels describing each brand
    L = []
    for level in levels:
        # Select level to process
        brands_classification_level = self.get_brand_classification([level])
        brands_classification_words_level = (
            brands_classification_level
            # List levels describing each brand
            .with_columns(
                pl.concat_list(
                    [
                        c
                        for c in brands_classification_level.columns
                        if c.startswith(&#34;level&#34;)
                    ]
                ).alias(f&#34;level&#34;)
            )
            .select(&#34;brand_desc_slug&#34;, &#34;level&#34;)
            .explode(f&#34;level&#34;)
            .filter(pl.col(f&#34;level&#34;).is_not_null())
        )
        # Add dataframe to the list of dataframes to merge
        L.append(brands_classification_words_level)

    brands_classification_words = (
        pl.concat(L)
        .groupby(&#34;brand_desc_slug&#34;)
        .agg(pl.col(f&#34;level&#34;))
        .with_columns(pl.col(f&#34;level&#34;).cast(pl.List(pl.Utf8)).list.join(&#34; &#34;))
    )

    # Keep significant words
    brands_classification_words_updated = DatasetsMerged.update_level_col(
        brands_classification_words, lemmatizer, list_stopwords
    )
    self.brand_classification_words = brands_classification_words_updated
    return self.brand_classification_words</code></pre>
</details>
</dd>
<dt id="fuzzy_matching.preprocessing.dataset.DatasetsMerged.pair_brands_with_same_products"><code class="name flex">
<span>def <span class="ident">pair_brands_with_same_products</span></span>(<span>self) ‑> polars.dataframe.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Pairs two brands related two the same product.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>datasets</code></strong> :&ensp;<code>list</code></dt>
<dd>list of dataframe containg products dans brands</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.DataFrame</code></dt>
<dd>a polars dataframe reporting pairs of brands</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pair_brands_with_same_products(self) -&gt; pl.DataFrame:
    &#34;&#34;&#34;Pairs two brands related two the same product.

    Args:
        datasets (list): list of dataframe containg products dans brands

    Returns:
        pl.DataFrame: a polars dataframe reporting pairs of brands
    &#34;&#34;&#34;
    # All possible pairs in List using combinations()
    pairs = list(combinations(self.sdf, 2))

    # Pairs two brands related two the same product
    datasets_paired = []
    for pair in pairs:
        datasets_paired.append(
            pl.concat(
                [
                    pair[0].select(
                        pl.col(&#34;product_id&#34;),
                        pl.col(&#34;brand_desc_slug&#34;).alias(&#34;brand_desc_left&#34;),
                    ),
                    pair[1].select(
                        pl.col(&#34;product_id&#34;),
                        pl.col(&#34;brand_desc_slug&#34;).alias(&#34;brand_desc_right&#34;),
                    ),
                ],
                how=&#34;align&#34;,
            )
            .filter(pl.col(&#34;brand_desc_left&#34;).is_not_null())
            .filter(pl.col(&#34;brand_desc_right&#34;).is_not_null())
            .filter(pl.col(&#34;brand_desc_left&#34;) != pl.col(&#34;brand_desc_right&#34;))
            .groupby(&#34;brand_desc_left&#34;, &#34;brand_desc_right&#34;)
            .count()
            .filter(pl.col(&#34;count&#34;) &gt; 1)
            .select(&#34;brand_desc_left&#34;, &#34;brand_desc_right&#34;)
        )

    datasets_paired_concat = pl.concat(datasets_paired, how=&#34;vertical&#34;)

    # Invert columns
    datasets_paired_concat_invert = (
        datasets_paired_concat.with_columns(pl.col(&#34;brand_desc_left&#34;).alias(&#34;tmp&#34;))
        .with_columns(pl.col(&#34;brand_desc_right&#34;).alias(&#34;brand_desc_left&#34;))
        .with_columns(pl.col(&#34;tmp&#34;).alias(&#34;brand_desc_right&#34;))
        .select(pl.col(&#34;brand_desc_left&#34;), pl.col(&#34;brand_desc_right&#34;))
    )

    return (
        pl.concat(
            [datasets_paired_concat, datasets_paired_concat_invert], how=&#34;vertical&#34;
        )
        .unique()
        .select(
            pl.col(&#34;brand_desc_left&#34;).alias(&#34;left_side&#34;),
            pl.col(&#34;brand_desc_right&#34;).alias(&#34;right_side&#34;),
        )
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fuzzy_matching.preprocessing" href="index.html">fuzzy_matching.preprocessing</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fuzzy_matching.preprocessing.dataset.Dataset" href="#fuzzy_matching.preprocessing.dataset.Dataset">Dataset</a></code></h4>
<ul class="">
<li><code><a title="fuzzy_matching.preprocessing.dataset.Dataset.clean_dataset" href="#fuzzy_matching.preprocessing.dataset.Dataset.clean_dataset">clean_dataset</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.Dataset.expand_levels" href="#fuzzy_matching.preprocessing.dataset.Dataset.expand_levels">expand_levels</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.Dataset.filter_dataset" href="#fuzzy_matching.preprocessing.dataset.Dataset.filter_dataset">filter_dataset</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.Dataset.upper_slug" href="#fuzzy_matching.preprocessing.dataset.Dataset.upper_slug">upper_slug</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged">DatasetsMerged</a></code></h4>
<ul class="">
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.cross_join" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.cross_join">cross_join</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.deduplicate_sentence" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.deduplicate_sentence">deduplicate_sentence</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.extract_brands" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.extract_brands">extract_brands</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification">get_brand_classification</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification_dummy" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification_dummy">get_brand_classification_dummy</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification_words" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.get_brand_classification_words">get_brand_classification_words</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.lemmatize_sentence" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.lemmatize_sentence">lemmatize_sentence</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.pair_brands_with_same_products" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.pair_brands_with_same_products">pair_brands_with_same_products</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_digit" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_digit">remove_digit</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_generic_words" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_generic_words">remove_generic_words</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_punctuation" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_punctuation">remove_punctuation</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_stopwords" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.remove_stopwords">remove_stopwords</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.update_brand_col" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.update_brand_col">update_brand_col</a></code></li>
<li><code><a title="fuzzy_matching.preprocessing.dataset.DatasetsMerged.update_level_col" href="#fuzzy_matching.preprocessing.dataset.DatasetsMerged.update_level_col">update_level_col</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>